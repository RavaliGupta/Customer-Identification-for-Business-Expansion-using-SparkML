{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('income_pred').getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["donor_df = spark.read.csv('/FileStore/tables/income_census.csv',inferSchema=True,header=True)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["donor_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["donor_df.show(5)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["donor_df = donor_df.filter(donor_df.income.isNotNull())"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["donor_df = donor_df.filter(donor_df.income.isNotNull())\ndonor_df = donor_df.withColumn('income',f.regexp_replace('income','>50K.','>50K')).withColumn('income',f.regexp_replace('income','<=50K.','<=50K'))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["### Total number of records in the dataset\ntotal_recs = donor_df.count()\nincome_abv50 = donor_df.filter(donor_df.income == '>50K').count()\nperc_income = (income_abv50/float(total_recs))*100\nprint('Total number of records in the dataset {}'.format(total_recs))\nprint('Number of individual records whose income is greater than $50000 : {}'.format(income_abv50))\nprint('Number of individual records whose income is  less than $50000 : {}'.format(donor_df.filter(donor_df.income == '<=50K').count()))\nprint('Percentage of individuals making more than $50,000 : {0:.2f}%'.format(perc_income))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["### Starting to preprocess the data. Renaming the columns in to usable format.\ndonor_df = donor_df.withColumnRenamed('education-num','education_num').withColumnRenamed('marital-status','marital_status').withColumnRenamed('capital-gain','capital_gain').withColumnRenamed('capital-loss','capital_loss').withColumnRenamed('hours-per-week','hours_per_week').withColumnRenamed('native-country','native_country')\n\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["max_capital_gain = donor_df.agg({\"capital_gain\": \"max\"}).collect()[0]\nmax_capital_gain = max_capital_gain['max(capital_gain)']\nmin_capital_gain = donor_df.agg({'capital_gain':'min'}).collect()[0]\nmin_capital_gain = min_capital_gain['min(capital_gain)']\nmax_capital_loss = donor_df.agg({'capital_loss':'max'}).collect()[0]\nmax_capital_loss = max_capital_loss['max(capital_loss)']\nmin_capital_loss = donor_df.agg({'capital_loss':'min'}).collect()[0]\nmin_capital_loss = min_capital_loss['min(capital_loss)']\nprint('Max and min values of capital-gain feature {} and {}'.format(max_capital_gain,min_capital_gain))\nprint('Max and min values of capital-loss feature {} and {}'.format(max_capital_loss,min_capital_loss))\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["### Applying log transformations on capital_gain and capital_loss features.\nimport pyspark.sql.functions as f\ndonor_df= donor_df.withColumn('capital_gain',f.log(donor_df.capital_gain + 1))\ndonor_df= donor_df.withColumn('capital_loss',f.log(donor_df.capital_loss + 1))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["## Applying the data transformation on other numeric features that is age,education_num, hours_per_week, capital_gain, capital_loss\n\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n\nmax_education_num , min_education_num = donor_df.select(f.max('education_num'), f.min('education_num')).first()\nmax_age, min_age = donor_df.select(f.max(\"age\"), f.min(\"age\")).first()\nmax_capital_gain, min_capital_gain = donor_df.select(f.max('capital_gain'),f.min('capital_gain')).first()\nmax_capital_loss , min_capital_loss = donor_df.select(f.max('capital_loss'), f.min('capital_loss')).first()\nmax_hours_per_week , min_hours_per_week = donor_df.select(f.max('hours_per_week'), f.min('hours_per_week')).first()\n\n"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["donor_df = donor_df.withColumn('age',(donor_df.age - min_age/max_age - min_age))\ndonor_df = donor_df.withColumn('capital_loss',(donor_df.capital_loss- min_capital_loss/max_capital_loss - min_capital_loss))\ndonor_df = donor_df.withColumn('capital_gain',(donor_df.capital_gain- min_capital_gain/max_capital_gain - min_capital_gain))\ndonor_df = donor_df.withColumn('hours_per_week',(donor_df.hours_per_week- min_hours_per_week/max_hours_per_week - min_hours_per_week))\ndonor_df = donor_df.withColumn('education_num',(donor_df.education_num - min_education_num/max_education_num - min_education_num))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["### Done preprocessing numeric data.\ndonor_df.select(donor_df.income).distinct().show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["indexer = StringIndexer(inputCol = 'income', outputCol = 'income_index')\ndonor_df= indexer.fit(donor_df).transform(donor_df)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["categorical_df = ['workclass','education_level','marital_status','occupation','relationship','race','sex','native_country']\nfor category in categorical_df:\n    indexer = StringIndexer(inputCol = category, outputCol = category+'_index')\n    donor_df = indexer.fit(donor_df).transform(donor_df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["for category in categorical_df:\n    encoder = OneHotEncoder(inputCol = category+'_index', outputCol = category+'_encoded' )\n    donor_df = encoder.transform(donor_df)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["donor_df.select('race_encoded','native_country_encoded','education_level_encoded').show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["donor_df.columns"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols=['age',\n  'education_num',\n  'capital_gain',\n 'capital_loss',\n 'hours_per_week',\n  'workclass_encoded',\n 'education_level_encoded',\n 'marital_status_encoded',\n 'occupation_encoded',\n 'relationship_encoded',\n 'race_encoded',\n 'sex_encoded',\n 'native_country_encoded'],outputCol='features')\ndonor_df = assembler.transform(donor_df)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["final_donor_df = donor_df.select('features','income_index')"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["final_donor_df = final_donor_df.withColumnRenamed('income_index','label')"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["final_donor_train , final_donor_test = final_donor_df.randomSplit([0.7,0.3])\n\nfinal_donor_train.show(5)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier,RandomForestClassifier, GBTClassifier\nfrom pyspark.ml.tuning import CrossValidator,ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["lrc = LogisticRegression(maxIter=10)  ## regParam=0.3, elasticNetParam=0.8\ndtc = DecisionTreeClassifier(labelCol='label',featuresCol='features')\nrfc = RandomForestClassifier(labelCol='label',featuresCol='features')  ### numTrees=10\ngbt = GBTClassifier(labelCol='label',featuresCol='features',maxIter=10)  "],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["bin_eval = BinaryClassificationEvaluator(labelCol='label')"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["params = ParamGridBuilder().addGrid(lrc.regParam, [0.1,0.01]).addGrid(lrc.elasticNetParam, [0,1]).addGrid(dtc.maxDepth, [1,5]).addGrid(rfc.numTrees, [1,10]).build()\nparams_dtc = ParamGridBuilder().addGrid(dtc.maxDepth, [1,5]).build()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["### Hyper parameter tuning and model selection\nestimators = ['lrc','rfc','dtc','gbt']\n\nfor clf in estimators:\n    cross_model = CrossValidator(estimator=dtc,estimatorParamMaps=params,evaluator=bin_eval,numFolds=5)\n    cv_model = cross_model.fit(final_donor_train)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["print('Best estimator is: ',cv_model.getEstimator())\nprint('Best estimator parameters: ',cv_model.bestModel)\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["### Making predictions on unseen test data with the best classifier.\ntest_preds = cv_model.transform(final_donor_test)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["test_preds.show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol='label')\n"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["### As this is an unbalanced dataset, accuracy is not the appropriate measure to evaluate the model. AreaunderROC will give us the proper evaluation.\nmetrics = evaluator.evaluate(test_preds,{evaluator.metricName:'areaUnderROC'})"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["print('Area under ROC curve: {0:.2f}'.format(metrics))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":34}],"metadata":{"name":"Customer Identification for Business Expansion","notebookId":601698659811114},"nbformat":4,"nbformat_minor":0}
